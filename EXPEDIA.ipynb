{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4556db6f-72a7-4df4-adac-2996ba11eca9",
   "metadata": {},
   "source": [
    "## Code for collecting Reddit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f35bac-6bd7-4eb0-8b96-3ee61e7a4fe6",
   "metadata": {},
   "source": [
    "**Code:**\n",
    "\n",
    "```python\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Reddit API with your credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"9jsUtejdbkYeYyFvxGXeXg\",  \n",
    "    client_secret=\"7q5QNOF5nrZZwNVOJy56nzs2Y36ngw\",  # Your Client Secret\n",
    "    user_agent=\"ExpediaAnalysis\"  \n",
    ")\n",
    "\n",
    "# Define competitors and subreddits for search\n",
    "subreddits = [\"travel\", \"Expedia\", \"Airbnb\", \"Booking\", \"Vrbo\", \"CustomerService\"]\n",
    "competitor_keywords = [\"Expedia\", \"Booking.com\", \"Vrbo\", \"Airbnb\"]\n",
    "limit = 1000  # **Number of posts to fetch per query**\n",
    "\n",
    "# Fetch data\n",
    "posts = []\n",
    "print(\"Fetching Reddit posts...\")\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    subreddit_obj = reddit.subreddit(subreddit)\n",
    "    print(f\"Searching subreddit: {subreddit}\")\n",
    "    \n",
    "    for query in competitor_keywords:\n",
    "        print(f\"  Searching for keyword: {query}\")\n",
    "        for post in subreddit_obj.search(query, limit=limit, time_filter=\"all\"):\n",
    "            posts.append({\n",
    "                \"Subreddit\": subreddit,\n",
    "                \"Title\": post.title[:100],  # **Truncate long titles**\n",
    "                \"Text\": (post.selftext[:200] + '...') if len(post.selftext) > 200 else post.selftext,  # **Truncate long texts**\n",
    "                \"Upvotes\": post.score,\n",
    "                \"Comments\": post.num_comments,\n",
    "                \"URL\": post.url,\n",
    "                \"Created\": pd.to_datetime(post.created_utc, unit=\"s\"),\n",
    "                \"Query\": query\n",
    "            })\n",
    "\n",
    "# Save data to a DataFrame\n",
    "data = pd.DataFrame(posts)\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "output_path = r'C:\\Users\\donia\\OneDrive\\Desktop\\HULT\\Buisiness Challenge\\Reddit_Competitor_Data.csv'\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "# Display summary of fetched posts\n",
    "print(\"\\nFetching complete!\")\n",
    "print(f\"Total posts fetched: {len(data)}\")\n",
    "print(f\"Data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba69c219-6c9f-4928-b696-3e9d68d43404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mDatasets loaded successfully\u001b[0m\n",
      "\n",
      "Main Reddit Competitor Data: 2735 rows\n",
      "VRBO Data: 100 rows\n",
      "Airbnb Data: 100 rows\n",
      "Expedia Data: 100 rows\n",
      "\n",
      "\u001b[1mReddit Posts per Competitor\u001b[0m\n",
      "\n",
      "Competitor\n",
      "Vrbo           828\n",
      "Airbnb         736\n",
      "Expedia        584\n",
      "Booking.com    395\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\u001b[1mFiltered Data Rows:\u001b[0m 2543\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  # For sentiment analysis\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define file paths\n",
    "file_path = r'C:\\Users\\donia\\OneDrive\\Desktop\\Buisiness Challenge\\Reddit_Competitor_Data.csv'\n",
    "\n",
    "# Load datasets\n",
    "data = pd.read_csv(file_path)  # Main Reddit competitor data\n",
    "vrbo_data = pd.read_csv('VRBO_CSV.csv')  # VRBO-specific data\n",
    "airbnb_data = pd.read_csv('AIRBNB_CSV.csv')  # Airbnb-specific data\n",
    "expedia_data = pd.read_csv('EXPEDIA_CSV.csv')  # Expedia-specific data\n",
    "\n",
    "# Print confirmation of dataset loading\n",
    "bold = \"\\033[1m\"  # ANSI code for bold text\n",
    "reset = \"\\033[0m\"  # ANSI code to reset text style\n",
    "\n",
    "print(f\"\\n{bold}Datasets loaded successfully{reset}\\n\")\n",
    "print(f\"Main Reddit Competitor Data: {data.shape[0]} rows\")\n",
    "print(f\"VRBO Data: {vrbo_data.shape[0]} rows\")\n",
    "print(f\"Airbnb Data: {airbnb_data.shape[0]} rows\")\n",
    "print(f\"Expedia Data: {expedia_data.shape[0]} rows\\n\")\n",
    "\n",
    "# Define competitor keywords and their names\n",
    "competitor_mapping = {\n",
    "    \"Expedia\": \"Expedia\",\n",
    "    \"Booking.com\": \"Booking.com\",\n",
    "    \"Vrbo\": \"Vrbo\",\n",
    "    \"Airbnb\": \"Airbnb\"\n",
    "}\n",
    "\n",
    "# Create a new column 'Competitor' based on mentions in Text or Title\n",
    "def identify_competitor(row):\n",
    "    # Check if any competitor keyword is found in the text or title of the post\n",
    "    for keyword, name in competitor_mapping.items():\n",
    "        if pd.notnull(row['Text']) and keyword.lower() in row['Text'].lower():\n",
    "            return name\n",
    "        if pd.notnull(row['Title']) and keyword.lower() in row['Title'].lower():\n",
    "            return name\n",
    "    return \"Unknown\"  # Assign 'Unknown' if no competitor is mentioned\n",
    "\n",
    "# Apply the function to the dataset\n",
    "data['Competitor'] = data.apply(identify_competitor, axis=1)\n",
    "\n",
    "# Filter out rows without a competitor mention\n",
    "data = data[data['Competitor'] != \"Unknown\"]\n",
    "\n",
    "# Display the number of posts per competitor\n",
    "print(f\"{bold}Reddit Posts per Competitor{reset}\\n\")\n",
    "competitor_counts = data['Competitor'].value_counts()\n",
    "print(competitor_counts)\n",
    "\n",
    "# Re-filter the data to ensure `filtered_data` is defined\n",
    "filtered_data = data[data['Competitor'] != \"Unknown\"]\n",
    "\n",
    "# Verify that `filtered_data` is properly defined\n",
    "print(f\"\\n{bold}Filtered Data Rows:{reset} {len(filtered_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b031d5b-c228-42a9-893c-77cfb7354956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mRefund/Cancellation Statistics by Competitor\u001b[0m\n",
      "\n",
      " Competitor  Total_Posts  Refund_Cancellation_Posts  Refund_Cancellation_Proportion (%)\n",
      "     Airbnb          679                        224                               32.99\n",
      "Booking.com          378                        153                               40.48\n",
      "    Expedia          542                        277                               51.11\n",
      "       Vrbo          799                        238                               29.79\n",
      "\n",
      "\u001b[1mRefund/Cancellation Issue Sentiment Analysis\u001b[0m\n",
      "\n",
      "SentimentCategory  Negative  Neutral  Positive  Total\n",
      "Competitor                                           \n",
      "Airbnb                47.32     1.79     50.89  100.0\n",
      "Booking.com           52.29     0.65     47.06  100.0\n",
      "Expedia               45.13     1.44     53.43  100.0\n",
      "Vrbo                  53.36     0.00     46.64  100.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  # For sentiment analysis\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# **Define ANSI escape codes for bold text**\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0m\"\n",
    "\n",
    "# Filter posts mentioning refund and cancellation-related issues\n",
    "data['RefundCancellationIssue'] = data['Text'].str.contains(\n",
    "    r'\\b(?:refund|refunded|non-refundable|cancel|cancelled|cancellation)\\b', \n",
    "    case=False, regex=True, na=False\n",
    ")\n",
    "\n",
    "# Group data by competitor and calculate refund/cancellation-related statistics\n",
    "competitor_stats = data.groupby('Competitor').agg(\n",
    "    Total_Posts=('Text', 'count'),\n",
    "    Refund_Cancellation_Posts=('RefundCancellationIssue', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the percentage of refund/cancellation-related posts\n",
    "competitor_stats['Refund_Cancellation_Proportion (%)'] = (\n",
    "    competitor_stats['Refund_Cancellation_Posts'] / competitor_stats['Total_Posts'] * 100\n",
    ")\n",
    "\n",
    "# Format and display the results\n",
    "print(f\"\\n{bold}Refund/Cancellation Statistics by Competitor{reset}\\n\")\n",
    "print(competitor_stats.to_string(index=False, float_format=\"{:.2f}\".format))\n",
    "\n",
    "# Apply sentiment analysis\n",
    "def vader_sentiment_category(text):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    scores = vader_analyzer.polarity_scores(text)\n",
    "    if scores['compound'] > 0.01:  # **Positive sentiment threshold**\n",
    "        return \"Positive\"\n",
    "    elif scores['compound'] < -0.01:  # **Negative sentiment threshold**\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Filter refund and cancellation-related posts\n",
    "refund_data = data.loc[data['RefundCancellationIssue']].copy()\n",
    "refund_data['SentimentCategory'] = refund_data['Text'].apply(vader_sentiment_category)\n",
    "\n",
    "# Summarize counts and percentages\n",
    "refund_sentiment_counts = refund_data.groupby(['Competitor', 'SentimentCategory'])['Text'].count().unstack(fill_value=0)\n",
    "refund_sentiment_counts['Total'] = refund_sentiment_counts.sum(axis=1)\n",
    "refund_sentiment_percentages = (refund_sentiment_counts.div(refund_sentiment_counts['Total'], axis=0) * 100).round(2)\n",
    "\n",
    "# Display sentiment analysis results\n",
    "print(f\"\\n{bold}Refund/Cancellation Issue Sentiment Analysis{reset}\\n\")\n",
    "print(refund_sentiment_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c45c709-d517-42e4-9708-aee62c1c14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mTrust Issue Percentages\u001b[0m\n",
      "\n",
      "SentimentCategory  Negative  Neutral  Positive  Total\n",
      "Competitor                                           \n",
      "Airbnb                51.75     1.40     46.85  100.0\n",
      "Booking.com           56.20     0.83     42.98  100.0\n",
      "Expedia               65.43     2.47     32.10  100.0\n",
      "Vrbo                  55.41     0.00     44.59  100.0\n",
      "\n",
      "\u001b[1mTrust Issue Counts\u001b[0m\n",
      "\n",
      "SentimentCategory  Negative  Neutral  Positive  Total\n",
      "Competitor                                           \n",
      "Airbnb                   74        2        67    143\n",
      "Booking.com              68        1        52    121\n",
      "Expedia                  53        2        26     81\n",
      "Vrbo                     82        0        66    148\n"
     ]
    }
   ],
   "source": [
    "# Define expanded keyword list for trust-related issues\n",
    "trust_keywords = [\n",
    "    # Scams and fraud\n",
    "    \"scam\", \"scams\", \"scammed\", \"fraud\", \"fraudulent\", \"fake\", \"sham\", \"phony\",\n",
    "\n",
    "    # Theft and dishonesty\n",
    "    \"stole\", \"stolen\", \"theft\", \"robbery\", \"stealing\", \"dishonest\", \"cheated\", \"lied\",\n",
    "\n",
    "    # Deceptive practices\n",
    "    \"deceptive\", \"misleading\", \"tricked\", \"false promises\", \"bait and switch\", \n",
    "    \"swindle\", \"ripped off\", \"dishonesty\", \"dishonest\",\n",
    "\n",
    "    # Broken trust\n",
    "    \"untrustworthy\", \"betrayed\", \"let down\", \"disappointed\", \"unreliable\",\n",
    "\n",
    "    # Rude behavior\n",
    "    \"rude\", \"unhelpful\", \"disrespectful\", \"arrogant\", \"impolite\", \"condescending\"\n",
    "]\n",
    "\n",
    "# Filter posts containing trust-related keywords\n",
    "data['TrustIssue'] = data['Text'].str.contains('|'.join(trust_keywords), case=False, na=False)\n",
    "\n",
    "# Apply VADER sentiment analysis\n",
    "def vader_sentiment_category(text):\n",
    "    scores = vader_analyzer.polarity_scores(text)\n",
    "    if scores['compound'] > 0.1:  # Positive threshold\n",
    "        return \"Positive\"\n",
    "    elif scores['compound'] < -0.1:  # **Negative threshold\n",
    "        return \"Negative\"\n",
    "    else:  # Neutral threshold\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply sentiment classification to filtered posts\n",
    "data['SentimentCategory'] = data.loc[data['TrustIssue'], 'Text'].apply(vader_sentiment_category)\n",
    "\n",
    "# **Count positive, neutral, and negative trust-related mentions by competitor\n",
    "trust_sentiment_counts = data.loc[data['TrustIssue']].groupby(\n",
    "    ['Competitor', 'SentimentCategory']\n",
    ")['Text'].count().unstack(fill_value=0)\n",
    "\n",
    "# Add totals and calculate percentages\n",
    "trust_sentiment_counts['Total'] = trust_sentiment_counts.sum(axis=1)\n",
    "trust_sentiment_percentages = (trust_sentiment_counts.div(\n",
    "    trust_sentiment_counts['Total'], axis=0) * 100).round(2)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{bold}Trust Issue Percentages{reset}\\n\")\n",
    "print(trust_sentiment_percentages)\n",
    "\n",
    "print(f\"\\n{bold}Trust Issue Counts{reset}\\n\")\n",
    "print(trust_sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b320841b-a2fb-4599-ad85-268c44b33b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPrice Difference Analysis by Cancellation Options\u001b[0m\n",
      "\n",
      "   Cancellation Options  No_Price_Difference (%)  Price_Difference (%)\n",
      "0                     0                   100.00                  0.00\n",
      "1                     1                    95.35                  4.65\n",
      "2                     2                     6.82                 93.18\n",
      "3                     3                     0.00                100.00\n",
      "4                     4                     0.00                100.00\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\donia\\OneDrive\\Desktop\\Buisiness Challenge\\EXPEDIA_CSV.csv\")\n",
    "\n",
    "# Group by Cancellation Options and calculate counts\n",
    "grouped = data.groupby('Cancellation Options').agg(\n",
    "    Total_Listings=('Price Difference', 'count'),\n",
    "    No_Price_Difference=('Price Difference', lambda x: (x == 0).sum()),\n",
    "    Price_Difference=('Price Difference', lambda x: (x != 0).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Calculate percentages\n",
    "grouped['No_Price_Difference (%)'] = (grouped['No_Price_Difference'] / grouped['Total_Listings'] * 100).round(2)\n",
    "grouped['Price_Difference (%)'] = (grouped['Price_Difference'] / grouped['Total_Listings'] * 100).round(2)\n",
    "\n",
    "# Keep relevant columns\n",
    "result = grouped[['Cancellation Options', 'No_Price_Difference (%)', 'Price_Difference (%)']]\n",
    "\n",
    "# Display the result\n",
    "print(f\"\\n{bold}Price Difference Analysis by Cancellation Options{reset}\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24b541c0-2d28-4707-9e7d-a31918b6352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mVRBO: Impact of Cancellation Policies on Review Scores \u001b[0m\n",
      "\n",
      "     Cancellation Avg_Review_Score  Count  % Listed Prior\n",
      "0  Non-Refundable              9.8      4          100.00\n",
      "1        Moderate         9.793333     30           90.00\n",
      "2         Relaxed         9.673077     52           86.54\n",
      "3  Partial Refund             7.95      4          100.00\n",
      "4           Total                -     90           88.89\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset file\n",
    "file_path = r\"C:\\Users\\donia\\OneDrive\\Desktop\\Buisiness Challenge\\VRBO_CSV.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter out rows with missing or zero review scores\n",
    "filtered_data = data[data['Review Score'] > 0]\n",
    "\n",
    "# Calculate properties with cancellation listed prior to booking\n",
    "total_properties = len(filtered_data)\n",
    "listed_prior_count = filtered_data['Listed Prior to Booking'].sum()\n",
    "percentage_listed_prior = (listed_prior_count / total_properties) * 100\n",
    "\n",
    "# Analyze the impact of Cancellation Policies on Review Scores\n",
    "cancellation_review_analysis = filtered_data.groupby('Cancellation').agg(\n",
    "    Avg_Review_Score=('Review Score', 'mean'),\n",
    "    Count=('Review Score', 'count'),\n",
    "    Listed_Prior_Count=('Listed Prior to Booking', 'sum')\n",
    ").reset_index().sort_values(by='Avg_Review_Score', ascending=False)\n",
    "\n",
    "# Add percentage of listed prior to booking\n",
    "cancellation_review_analysis['% Listed Prior'] = (\n",
    "    (cancellation_review_analysis['Listed_Prior_Count'] / cancellation_review_analysis['Count']) * 100\n",
    ").round(2)\n",
    "\n",
    "# Drop unnecessary column\n",
    "cancellation_review_analysis = cancellation_review_analysis.drop(columns=['Listed_Prior_Count'])\n",
    "\n",
    "# Add a Total row\n",
    "total_row = pd.DataFrame({\n",
    "    'Cancellation': ['Total'],\n",
    "    'Avg_Review_Score': ['-'],\n",
    "    'Count': [total_properties],\n",
    "    '% Listed Prior': [round(percentage_listed_prior, 2)]\n",
    "})\n",
    "\n",
    "cancellation_review_analysis = pd.concat([cancellation_review_analysis, total_row], ignore_index=True)\n",
    "\n",
    "# Print the analysis results\n",
    "print(f\"\\n{bold}VRBO: Impact of Cancellation Policies on Review Scores {reset}\\n\")\n",
    "print(cancellation_review_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d5a21b7-0759-44d1-a36e-42f9e9fe5380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAIRBNB: Impact of 'Amount of Times Listed' on Review Scores and Reviews:\u001b[0m\n",
      "\n",
      " Amount of Times Listed  Avg_Review_Score  Total_Reviews  Avg_Reviews_per_Property  Count_Properties\n",
      "                      2          4.934433          30761                317.123711                97\n",
      "                      3          4.970000            853                853.000000                 1\n",
      "\n",
      "\u001b[1mAIRBNB: Cancellation Policies and Amount of Times Listed:\u001b[0m\n",
      "\n",
      " Amount of Times Listed Cancellation  Avg_Review_Score  Total_Reviews  Count_Properties\n",
      "                      2     Flexible          4.945714           4500                14\n",
      "                      2      Limited          4.937778           3688                18\n",
      "                      2     Moderate          4.936053          13423                38\n",
      "                      2      Relaxed          4.906429           5412                14\n",
      "                      2       Strict          4.943077           3738                13\n",
      "                      3     Moderate          4.970000            853                 1\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\donia\\OneDrive\\Desktop\\Buisiness Challenge\\AIRBNB_CSV.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter rows with valid review scores\n",
    "filtered_data = data[data['Review Score'] > 0]\n",
    "\n",
    "# Define ANSI escape codes for bold text (if not already defined)\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0m\"\n",
    "\n",
    "\n",
    "# Amount of Times Listed on Review Scores\n",
    "\n",
    "amount_listed_stats = filtered_data.groupby('Amount of Times Listed').agg(\n",
    "    Avg_Review_Score=('Review Score', 'mean'),\n",
    "    Total_Reviews=('Count Reviews', 'sum'),\n",
    "    Avg_Reviews_per_Property=('Count Reviews', 'mean'),\n",
    "    Count_Properties=('property_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Display Results\n",
    "print(f\"\\n{bold}AIRBNB: Impact of 'Amount of Times Listed' on Review Scores and Reviews:{reset}\\n\")\n",
    "print(amount_listed_stats.to_string(index=False))\n",
    "\n",
    "\n",
    "# Cancellation Policy and Amount of Times Listed\n",
    "\n",
    "cancellation_amount_stats = filtered_data.groupby(['Amount of Times Listed', 'Cancellation']).agg(\n",
    "    Avg_Review_Score=('Review Score', 'mean'),\n",
    "    Total_Reviews=('Count Reviews', 'sum'),\n",
    "    Count_Properties=('property_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Display Results\n",
    "print(f\"\\n{bold}AIRBNB: Cancellation Policies and Amount of Times Listed:{reset}\\n\")\n",
    "print(cancellation_amount_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "836a3dc0-3012-4855-83f9-2b8e94bd82c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mImpact of Cancellation Timeline on Review Scores (Including Non-refundable):\u001b[0m\n",
      "\n",
      "Source   Cancellation         Cancellation Timeline  Avg_Review_Score  Total_Reviews\n",
      "Airbnb       Flexible                      48 Hours          4.945714           4500\n",
      "Airbnb        Limited 24 Hours, then non-refundable          4.937778           3688\n",
      "Airbnb       Moderate                        5 Days          4.936923          14276\n",
      "Airbnb        Relaxed                      24 Hours          4.906429           5412\n",
      "Airbnb         Strict                       30 Days          4.943077           3738\n",
      "  VRBO       Moderate                       30 Days          9.793333           2760\n",
      "  VRBO Non-refundable                 No exceptions          9.800000            623\n",
      "  VRBO Partial Refund                       14 Days          7.950000            361\n",
      "  VRBO        Relaxed                       14 Days          9.673077           3255\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "airbnb_path = r\"C:\\Users\\donia\\OneDrive\\Desktop\\Buisiness Challenge\\AIRBNB_CSV.csv\"\n",
    "vrbo_path = r\"C:\\Users\\donia\\OneDrive\\Desktop\\Buisiness Challenge\\VRBO_CSV.csv\"\n",
    "expedia_path = r\"C:\\Users\\donia\\OneDrive\\Desktop\\Buisiness Challenge\\EXPEDIA_CSV.csv\"\n",
    "\n",
    "# Load data\n",
    "airbnb_data = pd.read_csv(airbnb_path)\n",
    "vrbo_data = pd.read_csv(vrbo_path)\n",
    "expedia_data = pd.read_csv(expedia_path)\n",
    "\n",
    "# Define cancellation policy mappings\n",
    "airbnb_policy_mapping = {\n",
    "    'Relaxed': '24 Hours',\n",
    "    'Flexible': '48 Hours',\n",
    "    'Moderate': '5 Days',\n",
    "    'Strict': '30 Days',\n",
    "    'Limited': '24 Hours, then non-refundable'\n",
    "}\n",
    "vrbo_policy_mapping = {\n",
    "    'Relaxed': '14 Days',\n",
    "    'Moderate': '30 Days',\n",
    "    'Partial Refund': '14 Days',\n",
    "    'Non-refundable': 'No exceptions'\n",
    "}\n",
    "\n",
    "# Map cancellation policies to their timelines\n",
    "airbnb_data['Cancellation Timeline'] = airbnb_data['Cancellation'].map(airbnb_policy_mapping)\n",
    "vrbo_data['Cancellation Timeline'] = vrbo_data['Cancellation'].map(vrbo_policy_mapping)\n",
    "expedia_data['Cancellation Timeline'] = 'Not Specified'  # Placeholder for Expedia\n",
    "\n",
    "# Add a source column for tracking\n",
    "airbnb_data['Source'] = 'Airbnb'\n",
    "vrbo_data['Source'] = 'VRBO'\n",
    "expedia_data['Source'] = 'Expedia'\n",
    "\n",
    "# Manually handle non-refundable rows for VRBO\n",
    "non_refundable_rows = pd.DataFrame({\n",
    "    'Source': ['VRBO'] * 5,\n",
    "    'Cancellation': ['Non-refundable'] * 5,\n",
    "    'Cancellation Timeline': ['No exceptions'] * 5,\n",
    "    'Review Score': [10, 0, 10, 9.4, 9.8],  # Review scores for these rows\n",
    "    'Count Reviews': [122, 0, 471, 14, 16]  # Total reviews for these rows\n",
    "})\n",
    "\n",
    "# Append non-refundable rows to VRBO data\n",
    "vrbo_data = pd.concat([vrbo_data, non_refundable_rows], ignore_index=True)\n",
    "\n",
    "# Combine all datasets\n",
    "all_data = pd.concat([airbnb_data, vrbo_data, expedia_data], ignore_index=True)\n",
    "\n",
    "# Filter data with review scores > 0\n",
    "filtered_data = all_data[all_data['Review Score'] > 0]\n",
    "\n",
    "# Analyze the impact of cancellation timeline on review scores\n",
    "cancellation_analysis = filtered_data.groupby(\n",
    "    ['Source', 'Cancellation', 'Cancellation Timeline']\n",
    ").agg(\n",
    "    Avg_Review_Score=('Review Score', 'mean'),\n",
    "    Total_Reviews=('Count Reviews', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Sort by source and cancellation timeline\n",
    "cancellation_analysis = cancellation_analysis.sort_values(\n",
    "    ['Source', 'Cancellation'], ascending=[True, True]\n",
    ")\n",
    "\n",
    "# Display the final analysis\n",
    "print(f\"\\n{bold}Impact of Cancellation Timeline on Review Scores (Including Non-refundable):{reset}\\n\")\n",
    "print(cancellation_analysis.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed9642-0908-4c50-a2b7-175fc7f902a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
